{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"indoNLP indoNLP adalah library python sederhana tanpa dependency tambahan yang bertujuan untuk memudahkan proyek NLP anda. Installasi indoNLP dapat diinstall dengan mudah dengan menggunakan pip : $ pip install indoNLP","title":"indoNLP"},{"location":"#indonlp","text":"indoNLP adalah library python sederhana tanpa dependency tambahan yang bertujuan untuk memudahkan proyek NLP anda.","title":"indoNLP"},{"location":"#installasi","text":"indoNLP dapat diinstall dengan mudah dengan menggunakan pip : $ pip install indoNLP","title":"Installasi"},{"location":"changelog/","text":"Changelog v0.3.4 16 Oktober 2022 Bug Fixing Memperbaiki dan mengganti inner pattern pada fungsi replace_word_elongation . Updates Menambahkan wkwk pattern pada stopwords. Menggunakan inner flags untuk insensitive case. v0.3.3 30 September 2022 Bug Fixing Membenarkan bug pada pattern slang words yang disebabkan oleh common string pada SLANG_DATA . v0.3.2 04 September 2022 Update Mengubah return Dataset.read method menjadi dataclass Data . Update docstring pada code. v0.3.1 22 Agustus 2022 Documentation \ud83d\udcdd Membuat website dokumentasi untuk indoNLP menggunakan mkdocs dengan tema mkdocs-material dan menggenerasi kode referensi secara otomatis menggunakan mkdocstring . Mengubah kode docstring ke Bahasa Indonesia. Merge #3 ke master dan deploy dokumentasi menggunakan github action. Bug Fixing Memperbaiki top level import pada indoNLP/__init__.py Memperbaiki inconsistent return pada fungsi indoNLP.dataset.reader.txt_table_reader v0.3.0 17 Agustus 2022 New Features : Dataset \ud83d\udcd6 Modul baru yaitu indoNLP.dataset yang memudahkan cara mengakses open dataset pada kasus NLP dalam Bahasa Indonesia. v0.2.0 14 Juli 2022 Bug Fixing Memperbaiki bug pada fungsi preprocessing.replace_word_elongation yang mengganti kata berulang disetiap posisi pada sebuah kata menjadi di akhir kata saja. New Feature : Emoji Supports \ud83e\udd17 Preproses teks yang mengandung emoji kedalam Bahasa Indonesia dan sebaliknya. emoji_to_words words_to_emoji v0.1.1 30 Juni 2022 Fixing Membenarkan typo preprocessing.pipline menjadi preprocessing.pipeline v0.1.0 28 Juni 2022 Initial Release Membuat modul preprocessing yang terdiri dari beberapa fungsi. preprocessing.remove_html preprocessing.remove_url preprocessing.remove_stopwords preprocessing.replace_slang preprocessing.replace_word_elongation preprocessing.pipeline","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v034","text":"16 Oktober 2022 Bug Fixing Memperbaiki dan mengganti inner pattern pada fungsi replace_word_elongation . Updates Menambahkan wkwk pattern pada stopwords. Menggunakan inner flags untuk insensitive case.","title":"v0.3.4"},{"location":"changelog/#v033","text":"30 September 2022 Bug Fixing Membenarkan bug pada pattern slang words yang disebabkan oleh common string pada SLANG_DATA .","title":"v0.3.3"},{"location":"changelog/#v032","text":"04 September 2022 Update Mengubah return Dataset.read method menjadi dataclass Data . Update docstring pada code.","title":"v0.3.2"},{"location":"changelog/#v031","text":"22 Agustus 2022 Documentation \ud83d\udcdd Membuat website dokumentasi untuk indoNLP menggunakan mkdocs dengan tema mkdocs-material dan menggenerasi kode referensi secara otomatis menggunakan mkdocstring . Mengubah kode docstring ke Bahasa Indonesia. Merge #3 ke master dan deploy dokumentasi menggunakan github action. Bug Fixing Memperbaiki top level import pada indoNLP/__init__.py Memperbaiki inconsistent return pada fungsi indoNLP.dataset.reader.txt_table_reader","title":"v0.3.1"},{"location":"changelog/#v030","text":"17 Agustus 2022 New Features : Dataset \ud83d\udcd6 Modul baru yaitu indoNLP.dataset yang memudahkan cara mengakses open dataset pada kasus NLP dalam Bahasa Indonesia.","title":"v0.3.0"},{"location":"changelog/#v020","text":"14 Juli 2022 Bug Fixing Memperbaiki bug pada fungsi preprocessing.replace_word_elongation yang mengganti kata berulang disetiap posisi pada sebuah kata menjadi di akhir kata saja. New Feature : Emoji Supports \ud83e\udd17 Preproses teks yang mengandung emoji kedalam Bahasa Indonesia dan sebaliknya. emoji_to_words words_to_emoji","title":"v0.2.0"},{"location":"changelog/#v011","text":"30 Juni 2022 Fixing Membenarkan typo preprocessing.pipline menjadi preprocessing.pipeline","title":"v0.1.1"},{"location":"changelog/#v010","text":"28 Juni 2022 Initial Release Membuat modul preprocessing yang terdiri dari beberapa fungsi. preprocessing.remove_html preprocessing.remove_url preprocessing.remove_stopwords preprocessing.replace_slang preprocessing.replace_word_elongation preprocessing.pipeline","title":"v0.1.0"},{"location":"quickstart/","text":"Quick Start Mengakses Indonesian NLP Open Dataset Mengakses Indonesian NLP Open Dataset dengan cepat dan mudah. from indoNLP.dataset import Dataset handler = Dataset ( \"twitter-puisi\" ) data = handler . read () # out: Data(name='main', part_of='twitter-puisi') Mengecek kesimetrisan data, jika data bersifat simetrik maka data dapat ditabelisasi menggunakan pandas.DataFrame . import pandas as pd assert data . is_table (), \"Data tidak simetris, tidak dapat ditabulasi!\" df = pd . DataFrame ( data . data ) df . head () # out: # text # 0 Hanya karena sapa itu.\\nKau tikam rasamu.\\nSis... # 1 Sedang di antrian panjang\\nPada sebuah penanti... # 2 Jika kau bukan tempat awal untuk berlabuh, mak... # 3 Setiap waktu,\\nAku masih mendengar getar dawai... # 4 Sebait rindu yang kau bacakan\\nMasih terdengar... Preprocessing Data Teks Menerjemahkan emoji dan mengganti kata gaul ( slang words ). from indoNLP.preprocessing import emoji_to_words , replace_slang , pipeline pipe = pipeline ([ emoji_to_words , replace_slang ]) pipe ( \"library yg membara \ud83d\udd25\" ) # out: \"library yang membara !api!\"","title":"Quick Start"},{"location":"quickstart/#quick-start","text":"","title":"Quick Start"},{"location":"quickstart/#mengakses-indonesian-nlp-open-dataset","text":"Mengakses Indonesian NLP Open Dataset dengan cepat dan mudah. from indoNLP.dataset import Dataset handler = Dataset ( \"twitter-puisi\" ) data = handler . read () # out: Data(name='main', part_of='twitter-puisi') Mengecek kesimetrisan data, jika data bersifat simetrik maka data dapat ditabelisasi menggunakan pandas.DataFrame . import pandas as pd assert data . is_table (), \"Data tidak simetris, tidak dapat ditabulasi!\" df = pd . DataFrame ( data . data ) df . head () # out: # text # 0 Hanya karena sapa itu.\\nKau tikam rasamu.\\nSis... # 1 Sedang di antrian panjang\\nPada sebuah penanti... # 2 Jika kau bukan tempat awal untuk berlabuh, mak... # 3 Setiap waktu,\\nAku masih mendengar getar dawai... # 4 Sebait rindu yang kau bacakan\\nMasih terdengar...","title":"Mengakses Indonesian NLP Open Dataset"},{"location":"quickstart/#preprocessing-data-teks","text":"Menerjemahkan emoji dan mengganti kata gaul ( slang words ). from indoNLP.preprocessing import emoji_to_words , replace_slang , pipeline pipe = pipeline ([ emoji_to_words , replace_slang ]) pipe ( \"library yg membara \ud83d\udd25\" ) # out: \"library yang membara !api!\"","title":"Preprocessing Data Teks"},{"location":"api/","text":"Kode Referensi indoNLP memiliki beberapa modul yang dapat digunakan yaitu: dataset preprocessing","title":"Kode Referensi"},{"location":"api/#kode-referensi","text":"indoNLP memiliki beberapa modul yang dapat digunakan yaitu: dataset preprocessing","title":"Kode Referensi"},{"location":"api/dataset/","text":"indoNLP.dataset indoNLP.dataset adalah modul yang bertujuan untuk memudahkan mengakses open dataset dalam bidang NLP untuk Bahasa Indonesia. Dataset ( name , dataset_dir = None , auto_download = True ) Handler untuk dataset yang disupport indoNLP, berfungsi untuk mendownload, mengekstract, dan membaca data. Parameters: Name Type Description Default name str Nama dataset yang disupport indoNLP. required dataset_dir str indoNLP dataset download direktori. None auto_download bool Auto download dataset ketika kelas di inisiasi, jika dataset telah didownload sebelumnya maka proses download akan dilewati secara otomatis. True Attributes: Name Type Description dataset_name str Nama dataset yang disupport indoNLP. dataset_config Dict [ str , Any ] Konfigurasi dataset. file DatasetDirectoryHandler indoNLP dataset download direktori handler. downloader DataDownloader indoNLP dataset downloader. Examples: Download dan Loading dataset yang disupport. >>> handler = indoNLP . dataset . Dataset ( \"twitter-puisi\" ) >>> data = handler . read () >>> data Melihat dataset secara keseluruhan. >>> data . data cite () Mendapatkan cara mengutip dataset. Examples: >>> handler = indoNLP . dataset . Dataset ( \"id-abusive-language-detection\" ) >>> handler . cite () Ibrohim, M.O., Budi, I.. A Dataset and Preliminaries Study for Abusive Language Detection in Indonesian Social Media. Procedia Computer Science 2018;135:222-229. get_info () Mendapatkan informasi dari dataset. Informasi Menghasilkan output yang sama dengan fungsi get_supported_dataset_info . read ( get = 'all' ) Membaca file yang terdapat dalam dataset dan meloadnya kedalam memori. Jika data yang terdapat dalam file adalah simetric maka data dapat diload dengan menggunakan pandas.DataFrame . Parameters: Name Type Description Default get Union [ str , Tuple [ str ]] Nama file di dalam dataset untuk dibaca dan diload kedalam memori, jika \"all\" diset maka akan dibaca semua file yang ada di dalam dataset. 'all' Returns: Type Description Union [ Data , Tuple [ Data , ...]] Data dari file yang telah di read. Jika file yang dibaca berjumlah lebih dari 2 maka akan mengembalikan dalam bentuk Tuple sesuai dengan urutan nama file yang dispesifikasikan pada parameter get , jika \"all\" yang diberikan maka urutan data dari file akan sesuai dengan urutan yang ada pada metode get_info() . Examples: Membaca dan loading sebuah file. >>> handler = indoNLP . dataset . Dataset ( \"twitter-puisi\" ) >>> puisi = handler . read () twitter-puisi twitter-puisi dataset hanya memiliki 1 file didalamnya. Membaca beberapa file. >>> handler = indoNLP . dataset . Dataset ( \"asian-language-treebank-parallel-corpus\" ) >>> data_id , data_ja = handler . read ( get = ( \"id\" , \"ja\" )) asian-language-treebank-parallel-corpus asian-language-treebank-parallel-corpus dataset memiliki banyak file didalamnya. get_supported_dataset_list ( filter_tags = None ) Mendapatkan list dataset yang disupport oleh indoNLP. Parameters: Name Type Description Default filter_tags Union [ str , Sequence [ str ]] Filter dataset berdasarkan tags. None Informasi Untuk lebih lengkapnya list dataset yang disupport indoNLP dapat dilihat pada Supported Dataset . get_supported_dataset_info ( name ) Mendapatkan informasi terkait salah satu dataset yang disupport indoNLP. Parameters: Name Type Description Default name str Nama dataset yang dipilih. required Raises: Type Description KeyError Dataset tidak ditemukan (tidak disupport indoNLP). DataDownloader ( name , files , download_dir = None ) Dataset downloader, berfungsi untuk mendownload dan mengekstrak data secara langsung. Dapat digunakan untuk mendownload data yang tidak disupport oleh indoNLP dengan beberapa konfigurasi tambahan. Parameters: Name Type Description Default name str Nama dataset. required files List [ Dict [ str , str ]] List dari file - file yang terdapat pada dataset. Dictionary data harus mengandung elemen \"filename\" (str), \"url\" (str), dan \"extract\" (bool) agar proses dapat berjalan dengan baik. required download_dir Union [ str , DatasetDirectoryHandler ] indoNLP dataset download direktori handler. None Attributes: Name Type Description dataset_name str Nama dataset. dataset_files List [ Dict [ str , str ]] List dari file - file yang terdapat pada dataset. file DatasetDirectoryHandler indoNLP dataset download direktori handler. dataset_dir str Direktori tempat dataset didownload. Examples: Mendownload data yang tidak disupport oleh indoNLP . >>> downloader = indoNLP . dataset . downloader . DataDownloader ( ... \"msa-all-tab\" , ... files = [ ... { ... \"filename\" : \"Bahasa-Wordnet-master.zip\" , ... \"url\" : \"https://codeload.github.com/limaginaire/Bahasa-Wordnet/zip/refs/heads/master\" , ... \"is_large\" : True , ... \"extract\" : True , ... } ... ], ... download_dir = \"temp\" , ...) >>> downloader . download () check () Melakukan pengecekan apakah dataset masih tersedia pada url yang diberikan. Returns: Type Description List [ Dict [ str , Union [ str , int ]]] List status ketersediaan dari file - file di dataset. (status code) Examples: Mengecek ketersediaan dataset yang tidak disupport indoNLP di internet. >>> downloader = indoNLP . dataset . downloader . DataDownloader ( ... \"msa-all-tab\" , ... files = [ ... { ... \"filename\" : \"Bahasa-Wordnet-master.zip\" , ... \"url\" : \"https://codeload.github.com/limaginaire/Bahasa-Wordnet/zip/refs/heads/master\" , ... \"is_large\" : True , ... \"extract\" : True , ... } ... ], ... download_dir = \"temp\" , ...) >>> downloader . check () [{\"filename\": \"Bahasa-Wordnet-master.zip\", \"available\": True, \"status\": 200}] download () Mendownload dataset, proses ini dilakukan dengan mengiterasi setiap file yang ada di dalam dataset untuk di download. Proses ini termasuk dengan proses ekstraksi ketika file telah selesai di download dan extract=True terdapat pada dataset dictionary di parameter files . Note Proses ini hanya akan berjalan jika file - file dalam dataset belum pernah didownload sebelumnya. csv_reader ( path , fd_kwargs = {}, reader_kwargs = {}) csv file reader dimana data yang dikembalikan dapat langsung di pass kedalam pandas.DataFrame untuk melihat data dalam versi tablenya. Parameters: Name Type Description Default path str Path ke file csv. required fd_kwargs Dict [ str , Any ] File opener kwargs. {} reader_kwargs Dict [ str , Any ] Reader kwargs. {} Returns: Type Description Dict [ str , List [ Any ]] Data yang siap digunakan. txt_table_reader ( path , header = True , delimiter = ' \\t ' , fd_kwargs = {}) txt file reader dimana data yang dikembalikan dapat langsung di pass kedalam pandas.DataFrame untuk melihat data dalam versi tablenya. Parameters: Name Type Description Default path str Path ke file txt. required header bool Apakah data memiliki header, jika True diberikan maka baris pertama dari data akan dianggap sebagai header. True delimiter str Delimiter (pemisah). '\\t' fd_kwargs Dict [ str , Any ] File opener kwargs. {} Returns: Type Description Dict [ str , List [ Any ]] Data yang siap digunakan. jsonl_table_reader ( path , fd_kwargs = {}, reader_kwargs = {}) Symmetric jsonl file reader dimana data yang dikembalikan dapat langsung di pass kedalam pandas.DataFrame untuk melihat data dalam versi tablenya. Parameters: Name Type Description Default path str Path ke jsonl file. required fd_kwargs Dict [ str , Any ] File opener kwargs. {} reader_kwargs Dict [ str , Any ] Reader kwargs. {} Returns: Type Description Dict [ str , List [ Any ]] Data yang siap digunakan. Data dataclass Kelas yang menyimpan data utama setelah dibaca. Attributes: Name Type Description name str Nama data di dalam dataset. data Any Data yang telah dibaca. part_of str Dataset utama. table bool Apakah data bersifat simetrik? Note Jika nama yang diberikan adalah \"main\" maka data tersebut adalah data utama (data tunggal) dari dataset. is_table () Mengetahui apakah data bersifat simetrik. Informasi Jika dataset bersifat simetrik maka dataset dapat diload dalam bentuk tabel menggunakan kelas pandas.DataFrame . Returns: Type Description bool Kesimetrikan data. Examples: Melihat kesimetrikan data. >>> handler = indoNLP . dataset . Dataset ( \"twitter-puisi\" ) >>> data = handler . read () >>> data . is_table () True Loading data menggunakan pandas.DataFrame . >>> import pandas as pd >>> df = pd . DataFrame ( data . data ) DatasetDirectoryHandler ( download_dir = None ) Dataset directory handler, berfungsi untuk menghandle indoNLP main direktori untuk dataset. Secara otomatis data akan ditempatkan pada ~/.cache/indoNLP. Parameters: Name Type Description Default download_dir str Path ke main direktori untuk dataset. Jika None diset maka main direktori akan diarakan ke default yaitu ~/.cache/indoNLP. None Attributes: Name Type Description download_dir str Direktori indoNLP downloaded dataset. config_path str Konfigurasi file path. handler_config Dict [ str , Dict [ str , Any ]] indoNLP dataset configuration.","title":"Dataset"},{"location":"api/dataset/#indonlpdataset","text":"indoNLP.dataset adalah modul yang bertujuan untuk memudahkan mengakses open dataset dalam bidang NLP untuk Bahasa Indonesia.","title":"indoNLP.dataset"},{"location":"api/dataset/#indoNLP.dataset.Dataset","text":"Handler untuk dataset yang disupport indoNLP, berfungsi untuk mendownload, mengekstract, dan membaca data. Parameters: Name Type Description Default name str Nama dataset yang disupport indoNLP. required dataset_dir str indoNLP dataset download direktori. None auto_download bool Auto download dataset ketika kelas di inisiasi, jika dataset telah didownload sebelumnya maka proses download akan dilewati secara otomatis. True Attributes: Name Type Description dataset_name str Nama dataset yang disupport indoNLP. dataset_config Dict [ str , Any ] Konfigurasi dataset. file DatasetDirectoryHandler indoNLP dataset download direktori handler. downloader DataDownloader indoNLP dataset downloader. Examples: Download dan Loading dataset yang disupport. >>> handler = indoNLP . dataset . Dataset ( \"twitter-puisi\" ) >>> data = handler . read () >>> data Melihat dataset secara keseluruhan. >>> data . data","title":"Dataset"},{"location":"api/dataset/#indoNLP.dataset.Dataset.cite","text":"Mendapatkan cara mengutip dataset. Examples: >>> handler = indoNLP . dataset . Dataset ( \"id-abusive-language-detection\" ) >>> handler . cite () Ibrohim, M.O., Budi, I.. A Dataset and Preliminaries Study for Abusive Language Detection in Indonesian Social Media. Procedia Computer Science 2018;135:222-229.","title":"cite()"},{"location":"api/dataset/#indoNLP.dataset.Dataset.get_info","text":"Mendapatkan informasi dari dataset. Informasi Menghasilkan output yang sama dengan fungsi get_supported_dataset_info .","title":"get_info()"},{"location":"api/dataset/#indoNLP.dataset.Dataset.read","text":"Membaca file yang terdapat dalam dataset dan meloadnya kedalam memori. Jika data yang terdapat dalam file adalah simetric maka data dapat diload dengan menggunakan pandas.DataFrame . Parameters: Name Type Description Default get Union [ str , Tuple [ str ]] Nama file di dalam dataset untuk dibaca dan diload kedalam memori, jika \"all\" diset maka akan dibaca semua file yang ada di dalam dataset. 'all' Returns: Type Description Union [ Data , Tuple [ Data , ...]] Data dari file yang telah di read. Jika file yang dibaca berjumlah lebih dari 2 maka akan mengembalikan dalam bentuk Tuple sesuai dengan urutan nama file yang dispesifikasikan pada parameter get , jika \"all\" yang diberikan maka urutan data dari file akan sesuai dengan urutan yang ada pada metode get_info() . Examples: Membaca dan loading sebuah file. >>> handler = indoNLP . dataset . Dataset ( \"twitter-puisi\" ) >>> puisi = handler . read () twitter-puisi twitter-puisi dataset hanya memiliki 1 file didalamnya. Membaca beberapa file. >>> handler = indoNLP . dataset . Dataset ( \"asian-language-treebank-parallel-corpus\" ) >>> data_id , data_ja = handler . read ( get = ( \"id\" , \"ja\" )) asian-language-treebank-parallel-corpus asian-language-treebank-parallel-corpus dataset memiliki banyak file didalamnya.","title":"read()"},{"location":"api/dataset/#indoNLP.dataset.get_supported_dataset_list","text":"Mendapatkan list dataset yang disupport oleh indoNLP. Parameters: Name Type Description Default filter_tags Union [ str , Sequence [ str ]] Filter dataset berdasarkan tags. None Informasi Untuk lebih lengkapnya list dataset yang disupport indoNLP dapat dilihat pada Supported Dataset .","title":"get_supported_dataset_list()"},{"location":"api/dataset/#indoNLP.dataset.get_supported_dataset_info","text":"Mendapatkan informasi terkait salah satu dataset yang disupport indoNLP. Parameters: Name Type Description Default name str Nama dataset yang dipilih. required Raises: Type Description KeyError Dataset tidak ditemukan (tidak disupport indoNLP).","title":"get_supported_dataset_info()"},{"location":"api/dataset/#indoNLP.dataset.downloader.DataDownloader","text":"Dataset downloader, berfungsi untuk mendownload dan mengekstrak data secara langsung. Dapat digunakan untuk mendownload data yang tidak disupport oleh indoNLP dengan beberapa konfigurasi tambahan. Parameters: Name Type Description Default name str Nama dataset. required files List [ Dict [ str , str ]] List dari file - file yang terdapat pada dataset. Dictionary data harus mengandung elemen \"filename\" (str), \"url\" (str), dan \"extract\" (bool) agar proses dapat berjalan dengan baik. required download_dir Union [ str , DatasetDirectoryHandler ] indoNLP dataset download direktori handler. None Attributes: Name Type Description dataset_name str Nama dataset. dataset_files List [ Dict [ str , str ]] List dari file - file yang terdapat pada dataset. file DatasetDirectoryHandler indoNLP dataset download direktori handler. dataset_dir str Direktori tempat dataset didownload. Examples: Mendownload data yang tidak disupport oleh indoNLP . >>> downloader = indoNLP . dataset . downloader . DataDownloader ( ... \"msa-all-tab\" , ... files = [ ... { ... \"filename\" : \"Bahasa-Wordnet-master.zip\" , ... \"url\" : \"https://codeload.github.com/limaginaire/Bahasa-Wordnet/zip/refs/heads/master\" , ... \"is_large\" : True , ... \"extract\" : True , ... } ... ], ... download_dir = \"temp\" , ...) >>> downloader . download ()","title":"DataDownloader"},{"location":"api/dataset/#indoNLP.dataset.downloader.DataDownloader.check","text":"Melakukan pengecekan apakah dataset masih tersedia pada url yang diberikan. Returns: Type Description List [ Dict [ str , Union [ str , int ]]] List status ketersediaan dari file - file di dataset. (status code) Examples: Mengecek ketersediaan dataset yang tidak disupport indoNLP di internet. >>> downloader = indoNLP . dataset . downloader . DataDownloader ( ... \"msa-all-tab\" , ... files = [ ... { ... \"filename\" : \"Bahasa-Wordnet-master.zip\" , ... \"url\" : \"https://codeload.github.com/limaginaire/Bahasa-Wordnet/zip/refs/heads/master\" , ... \"is_large\" : True , ... \"extract\" : True , ... } ... ], ... download_dir = \"temp\" , ...) >>> downloader . check () [{\"filename\": \"Bahasa-Wordnet-master.zip\", \"available\": True, \"status\": 200}]","title":"check()"},{"location":"api/dataset/#indoNLP.dataset.downloader.DataDownloader.download","text":"Mendownload dataset, proses ini dilakukan dengan mengiterasi setiap file yang ada di dalam dataset untuk di download. Proses ini termasuk dengan proses ekstraksi ketika file telah selesai di download dan extract=True terdapat pada dataset dictionary di parameter files . Note Proses ini hanya akan berjalan jika file - file dalam dataset belum pernah didownload sebelumnya.","title":"download()"},{"location":"api/dataset/#indoNLP.dataset.reader.csv_reader","text":"csv file reader dimana data yang dikembalikan dapat langsung di pass kedalam pandas.DataFrame untuk melihat data dalam versi tablenya. Parameters: Name Type Description Default path str Path ke file csv. required fd_kwargs Dict [ str , Any ] File opener kwargs. {} reader_kwargs Dict [ str , Any ] Reader kwargs. {} Returns: Type Description Dict [ str , List [ Any ]] Data yang siap digunakan.","title":"csv_reader()"},{"location":"api/dataset/#indoNLP.dataset.reader.txt_table_reader","text":"txt file reader dimana data yang dikembalikan dapat langsung di pass kedalam pandas.DataFrame untuk melihat data dalam versi tablenya. Parameters: Name Type Description Default path str Path ke file txt. required header bool Apakah data memiliki header, jika True diberikan maka baris pertama dari data akan dianggap sebagai header. True delimiter str Delimiter (pemisah). '\\t' fd_kwargs Dict [ str , Any ] File opener kwargs. {} Returns: Type Description Dict [ str , List [ Any ]] Data yang siap digunakan.","title":"txt_table_reader()"},{"location":"api/dataset/#indoNLP.dataset.reader.jsonl_table_reader","text":"Symmetric jsonl file reader dimana data yang dikembalikan dapat langsung di pass kedalam pandas.DataFrame untuk melihat data dalam versi tablenya. Parameters: Name Type Description Default path str Path ke jsonl file. required fd_kwargs Dict [ str , Any ] File opener kwargs. {} reader_kwargs Dict [ str , Any ] Reader kwargs. {} Returns: Type Description Dict [ str , List [ Any ]] Data yang siap digunakan.","title":"jsonl_table_reader()"},{"location":"api/dataset/#indoNLP.dataset.utils.Data","text":"Kelas yang menyimpan data utama setelah dibaca. Attributes: Name Type Description name str Nama data di dalam dataset. data Any Data yang telah dibaca. part_of str Dataset utama. table bool Apakah data bersifat simetrik? Note Jika nama yang diberikan adalah \"main\" maka data tersebut adalah data utama (data tunggal) dari dataset.","title":"Data"},{"location":"api/dataset/#indoNLP.dataset.utils.Data.is_table","text":"Mengetahui apakah data bersifat simetrik. Informasi Jika dataset bersifat simetrik maka dataset dapat diload dalam bentuk tabel menggunakan kelas pandas.DataFrame . Returns: Type Description bool Kesimetrikan data. Examples: Melihat kesimetrikan data. >>> handler = indoNLP . dataset . Dataset ( \"twitter-puisi\" ) >>> data = handler . read () >>> data . is_table () True Loading data menggunakan pandas.DataFrame . >>> import pandas as pd >>> df = pd . DataFrame ( data . data )","title":"is_table()"},{"location":"api/dataset/#indoNLP.dataset.utils.DatasetDirectoryHandler","text":"Dataset directory handler, berfungsi untuk menghandle indoNLP main direktori untuk dataset. Secara otomatis data akan ditempatkan pada ~/.cache/indoNLP. Parameters: Name Type Description Default download_dir str Path ke main direktori untuk dataset. Jika None diset maka main direktori akan diarakan ke default yaitu ~/.cache/indoNLP. None Attributes: Name Type Description download_dir str Direktori indoNLP downloaded dataset. config_path str Konfigurasi file path. handler_config Dict [ str , Dict [ str , Any ]] indoNLP dataset configuration.","title":"DatasetDirectoryHandler"},{"location":"api/dataset/sup-dataset/","text":"Supported Dataset Berikut adalah daftar dataset yang disupport oleh indoNLP . Perhatian Disupport disini dimaksudkan sebagai dataset yang dapat digunakan secara langsung tanpa konfigurasi tambahan oleh indoNLP . indoNLP tidak memiliki hak cipta apapun terkait dataset yang ada di daftar! twitter-puisi Puisi - puisi yang difilter dari beberbagai pengguna di Twitter. Homepage unlabeled id-multi-label-hate-speech-and-abusive-language-detection Muhammad Okky Ibrohim dan Indra Budi - 2019 Dataset untuk pembelajaran multi-label tentang hate speech dan abusive language detection dari berbagai tweet di Twitter. Homepage Cite Muhammad Okky Ibrohim and Indra Budi. 2019. Multi-label Hate Speech and Abusive Language Detection in Indonesian Twitter. In ALW3: 3rd Workshop on Abusive Language Online, 46-57. abusive language detection, hate speech, labeled, multi-label, twitter id-abusive-language-detection Muhammad Okky Ibrohim dan Indra Budi - 2018 Dataset untuk pembelajaran multi-label tentang abusive language detection pada Bahasa Indonesia. Homepage Cite Ibrohim, M.O., Budi, I.. A Dataset and Preliminaries Study for Abusive Language Detection in Indonesian Social Media. Procedia Computer Science 2018;135:222-229. abusive language detection, labeled asian-language-treebank-parallel-corpus Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah Mahani Aljunied, Luong Chi Mai, Vu Tat Thang, Nguyen Phuong Thai, Vichet Chea, Rapid Sun, Sethserey Sam, Sopheap Seng, Khin Mar Soe, Khin Thandar Nwet, Masao Utiyama, dan Chenchen Ding - 2016 Proyek ALT adalah proyek yang bertujuan untuk memajukan teknik NLP pada bahasa - bahasa di Asia melalui kolaborasi terbuka. Proses membangun ALT dimulai dengan mengambil sampel sekitar 20.000 kalimat dari Wikinews bahasa Inggris, dan kemudian diterjemahkan ke dalam bahasa lain. Homepage Cite Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah Mahani Aljunied, Luong Chi Mai, Vu Tat Thang, Nguyen Phuong Thai, Vichet Chea, Rapid Sun, Sethserey Sam, Sopheap Seng, Khin Mar Soe, Khin Thandar Nwet, Masao Utiyama, Chenchen Ding. (2016) 'Introduction of the Asian Language Treebank' Oriental COCOSDA. machine translation","title":"Supported Dataset"},{"location":"api/dataset/sup-dataset/#supported-dataset","text":"Berikut adalah daftar dataset yang disupport oleh indoNLP . Perhatian Disupport disini dimaksudkan sebagai dataset yang dapat digunakan secara langsung tanpa konfigurasi tambahan oleh indoNLP . indoNLP tidak memiliki hak cipta apapun terkait dataset yang ada di daftar!","title":"Supported Dataset"},{"location":"api/dataset/sup-dataset/#twitter-puisi","text":"Puisi - puisi yang difilter dari beberbagai pengguna di Twitter. Homepage unlabeled","title":"twitter-puisi"},{"location":"api/dataset/sup-dataset/#id-multi-label-hate-speech-and-abusive-language-detection","text":"Muhammad Okky Ibrohim dan Indra Budi - 2019 Dataset untuk pembelajaran multi-label tentang hate speech dan abusive language detection dari berbagai tweet di Twitter. Homepage Cite Muhammad Okky Ibrohim and Indra Budi. 2019. Multi-label Hate Speech and Abusive Language Detection in Indonesian Twitter. In ALW3: 3rd Workshop on Abusive Language Online, 46-57. abusive language detection, hate speech, labeled, multi-label, twitter","title":"id-multi-label-hate-speech-and-abusive-language-detection"},{"location":"api/dataset/sup-dataset/#id-abusive-language-detection","text":"Muhammad Okky Ibrohim dan Indra Budi - 2018 Dataset untuk pembelajaran multi-label tentang abusive language detection pada Bahasa Indonesia. Homepage Cite Ibrohim, M.O., Budi, I.. A Dataset and Preliminaries Study for Abusive Language Detection in Indonesian Social Media. Procedia Computer Science 2018;135:222-229. abusive language detection, labeled","title":"id-abusive-language-detection"},{"location":"api/dataset/sup-dataset/#asian-language-treebank-parallel-corpus","text":"Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah Mahani Aljunied, Luong Chi Mai, Vu Tat Thang, Nguyen Phuong Thai, Vichet Chea, Rapid Sun, Sethserey Sam, Sopheap Seng, Khin Mar Soe, Khin Thandar Nwet, Masao Utiyama, dan Chenchen Ding - 2016 Proyek ALT adalah proyek yang bertujuan untuk memajukan teknik NLP pada bahasa - bahasa di Asia melalui kolaborasi terbuka. Proses membangun ALT dimulai dengan mengambil sampel sekitar 20.000 kalimat dari Wikinews bahasa Inggris, dan kemudian diterjemahkan ke dalam bahasa lain. Homepage Cite Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah Mahani Aljunied, Luong Chi Mai, Vu Tat Thang, Nguyen Phuong Thai, Vichet Chea, Rapid Sun, Sethserey Sam, Sopheap Seng, Khin Mar Soe, Khin Thandar Nwet, Masao Utiyama, Chenchen Ding. (2016) 'Introduction of the Asian Language Treebank' Oriental COCOSDA. machine translation","title":"asian-language-treebank-parallel-corpus"},{"location":"api/preprocessing/","text":"indoNLP.preprocessing indoNLP.preprocessing adalah modul yang bertujuan untuk memudahkan proses preprocessing data teks dengan menggunakan beberapa fungsi yang siap digunakan. remove_html ( text ) Menghapus tag - tag html yang terdapat dalam sebuah teks. Parameters: Name Type Description Default text str Teks yang memiliki html tag di dalamnya. required Returns: Type Description str Teks yang telah dibersihkan (tanpa tag - tag HTML di dalamnya). Examples: Menghapus semua tag HTML yang terdapat di dalam teks. >>> indoNLP . preprocessing . remove_html ( \"website <a href='https://google.com'>google</a>\" ) \"website google\" remove_url ( text ) Menghapus URL yang terdapat dalam sebuah teks. Parameters: Name Type Description Default text str Teks yang terdapat URL di dalamnya. required Returns: Type Description str Teks yang telah dibersihkan (tanpa URL di dalamnya). Examples: Menghapus semua URL yang ada di dalam teks. >>> indoNLP . preprocessing . remove_url ( \"retrieved from https://gist.github.com/gruber/8891611\" ) \"retrieved from\" remove_stopwords ( text ) Menghapus stopwords yang terdapat dalam sebuah teks. Definisi Stopwords adalah kata umum ( common words ) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Sumber List stopwords Bahasa Indonesia yang digunakan diperoleh dari stopwords.net Parameters: Name Type Description Default text str Teks yang terdapat stopwords di dalamnya. required Returns: Type Description str Teks yang telah dibersihkan (tanpa stopwords di dalamnya). Examples: Menghapus semua stopwords yang terdapat di dalam sebuah teks. >>> indoNLP . preprocessing . remove_stopwords ( \"siapa yang suruh makan?!!\" ) \"suruh makan?!!\" replace_slang ( text ) Menghapus slang words (kata gaul) yang terdapat dalam sebuah teks. Kata gaul dapat juga berupa singkatan yang sering digunakan dalam kehidupan sehari - hari seperti: \"yg\" -> \"yang\" \"mkn\" -> \"makan\" Sumber Mapper untuk slang words yang digunakan didapatkan dari Kamus Alay - Colloquial Indonesian Lexicon oleh Salsabila, Ali, Yosef, dan Ade. Parameters: Name Type Description Default text str Teks yang terdapat slang words di dalamnya. required Returns: Type Description str Teks yang telah dimodifikasi (tanpa slang words di dalamnya). Examples: Mengganti setiap slang words yang ada di dalam teks menjadi bentuk yang lebih formal. >>> indoNLP . preprocessing . replace_slang ( \"emg siapa yg nanya?\" ) \"memang siapa yang bertanya?\" replace_word_elongation ( text ) Mengganti word elongation yang terdapat pada sebuah teks. Definisi Word elongation adalah tindakan menambahkan huruf tambahan ke kata, biasanya terdapat di akhir kata, hal ini biasanya dilakukan agar terdengar lebih ceria, ramah, dan imut. Parameters: Name Type Description Default text str Teks yang terdapat word elongation di dalamnya. required Returns: Type Description str Teks yang telah ditransformasi (tanpa word elongation ). Examples: Mengganti setiap word elongation yang terdapat pada sebuah teks. >>> indoNLP . preprocessing . replace_word_elongation ( \"kenapaaa?\" ) \"kenapa?\" pipeline ( pipe ) Pipelining fungsi preprocessing. Parameters: Name Type Description Default pipe Sequence [ Callable [[ str ], str ]] Sequence dari fungsi - fungsi preprocessing indoNLP . required Returns: Type Description Callable [[ str ], str ] Callable pipeline. Examples: Pipelining beberapa fungsi preprocessing. >>> from indoNLP.preprocessing import pipeline , replace_word_elongation , replace_slang >>> pipe = pipeline ([ replace_word_elongation , replace_slang ]) >>> pipe ( \"Knp emg gk mw makan kenapaaa???\" ) \"kenapa memang enggak mau makan kenapa???\" emoji_to_words ( text , lang = 'id' , use_alias = False , delimiter = ( '!' , '!' )) Transformasi emoji yang ada di dalam teks menjadi kata - kata yang sesuai dengan emoji tersebut dalam Bahasa Indonesia. Parameters: Name Type Description Default text str Teks yang terdapat emoji di dalamnya. required lang str Kode bahasa, bahasa yang tersedia yaitu \"en\" (English) dan \"id\" (Bahasa Indonesia). 'id' use_alias bool Menggunakan alias translation, alias adalah terjemahan yang lebih spesifik terhadap emoji tersebut. Tidak setiap emoji memiliki alias dan use_alias hanya didukung untuk Bahasa Indonesia lang=\"id\" . False delimiter Tuple [ str , str ] Delimiter (pembatas) pada terjemahan emoji, berupa tupple dengan dua element string sebagai pembatas awal dan akhir. ('!', '!') Warning Jika use_alias == True and lang != \"id\" maka akan terjadi error. Returns: Type Description str Teks yang telah di transformasi atau tidak terdapat emoji di dalamnya dan telah digantikan str dengan kata - kata yang mengekspresikan emoji tersebut. Examples: Mentransformasi emoji kedalam Bahasa Indonesia. >>> indoNLP . preprocessing . emoji_to_words ( \"emoji \ud83d\ude00\ud83d\ude01\" ) \"emoji !wajah_gembira!!wajah_gembira_dengan_mata_bahagia!\" Mentransformasi emoji kebahasa Ingris. >>> indoNLP . preprocessing . emoji_to_words ( \"emoji \ud83d\ude00\ud83d\ude01\" , lang = \"en\" ) \"emoji !grinning_face!!beaming_face_with_smiling_eyes!\" Menggunakan alias. >>> indoNLP . preprocessing . emoji_to_words ( \"emoji \ud83d\ude00\" , use_alias = True ) \"emoji !wajah_gembira_bahagia_muka_senang!\" Menggunakan custom delimiter. >>> indoNLP . preprocessing . emoji_to_words ( \"emoji \ud83d\ude01\" , delimiter = ( \"^\" , \"$\" )) \"emoji ^wajah_gembira_dengan_mata_bahagia$\" words_to_emoji ( text , lang = 'id' , use_alias = False , delimiter = ( '!' , '!' )) Transformasi kata - kata dengan kode emoji menjadi emoji. Parameters: Name Type Description Default text str Teks yang terdapat kata - kata dengan kode emoji di dalamnya. required lang str Kode bahasa, bahasa yang tersedia yaitu \"en\" (English) dan \"id\" (Bahasa Indonesia). 'id' use_alias bool Menggunakan alias translation, alias adalah terjemahan yang lebih spesifik terhadap emoji tersebut. Tidak setiap emoji memiliki alias dan use_alias hanya didukung untuk Bahasa Indonesia lang=\"id\" . False delimiter Tuple [ str , str ] Delimiter (pembatas) pada kata - kata kode emoji, berupa tupple dengan dua element string sebagai pembatas awal dan akhir. ('!', '!') Returns: Type Description str Teks yang telah di transformasi atau kata - kata yang mengandung kode emoji di dalam teks str telah diubah menjadi emooji. Examples: Transformasi kata - kata kode emoji di dalam teks menjadi emoji. >>> indoNLP . preprocessing . emoji_to_words ( \"emoji !wajah_gembira!!wajah_gembira_dengan_mata_bahagia!\" ) \"emoji \ud83d\ude00\ud83d\ude01\" Transform english words to emoji >>> indoNLP . preprocessing . emoji_to_words ( \"emoji !beaming_face_with_smiling_eyes!\" , lang = \"en\" ) \"emoji \ud83d\ude01\" Using alias. Only works on lang == \"id\" >>> indoNLP . preprocessing . emoji_to_words ( \"emoji !wajah_gembira_bahagia_muka_senang!\" , use_alias = True ) \"emoji \ud83d\ude00\" Using custom delimiter >>> indoNLP . preprocessing . emoji_to_words ( \"emoji ^wajah_gembira_dengan_mata_bahagia$\" , delimiter = ( \"^\" , \"$\" )) \"emoji \ud83d\ude01\"","title":"Preprocessing"},{"location":"api/preprocessing/#indonlppreprocessing","text":"indoNLP.preprocessing adalah modul yang bertujuan untuk memudahkan proses preprocessing data teks dengan menggunakan beberapa fungsi yang siap digunakan.","title":"indoNLP.preprocessing"},{"location":"api/preprocessing/#indoNLP.preprocessing.remove_html","text":"Menghapus tag - tag html yang terdapat dalam sebuah teks. Parameters: Name Type Description Default text str Teks yang memiliki html tag di dalamnya. required Returns: Type Description str Teks yang telah dibersihkan (tanpa tag - tag HTML di dalamnya). Examples: Menghapus semua tag HTML yang terdapat di dalam teks. >>> indoNLP . preprocessing . remove_html ( \"website <a href='https://google.com'>google</a>\" ) \"website google\"","title":"remove_html()"},{"location":"api/preprocessing/#indoNLP.preprocessing.remove_url","text":"Menghapus URL yang terdapat dalam sebuah teks. Parameters: Name Type Description Default text str Teks yang terdapat URL di dalamnya. required Returns: Type Description str Teks yang telah dibersihkan (tanpa URL di dalamnya). Examples: Menghapus semua URL yang ada di dalam teks. >>> indoNLP . preprocessing . remove_url ( \"retrieved from https://gist.github.com/gruber/8891611\" ) \"retrieved from\"","title":"remove_url()"},{"location":"api/preprocessing/#indoNLP.preprocessing.remove_stopwords","text":"Menghapus stopwords yang terdapat dalam sebuah teks. Definisi Stopwords adalah kata umum ( common words ) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Sumber List stopwords Bahasa Indonesia yang digunakan diperoleh dari stopwords.net Parameters: Name Type Description Default text str Teks yang terdapat stopwords di dalamnya. required Returns: Type Description str Teks yang telah dibersihkan (tanpa stopwords di dalamnya). Examples: Menghapus semua stopwords yang terdapat di dalam sebuah teks. >>> indoNLP . preprocessing . remove_stopwords ( \"siapa yang suruh makan?!!\" ) \"suruh makan?!!\"","title":"remove_stopwords()"},{"location":"api/preprocessing/#indoNLP.preprocessing.replace_slang","text":"Menghapus slang words (kata gaul) yang terdapat dalam sebuah teks. Kata gaul dapat juga berupa singkatan yang sering digunakan dalam kehidupan sehari - hari seperti: \"yg\" -> \"yang\" \"mkn\" -> \"makan\" Sumber Mapper untuk slang words yang digunakan didapatkan dari Kamus Alay - Colloquial Indonesian Lexicon oleh Salsabila, Ali, Yosef, dan Ade. Parameters: Name Type Description Default text str Teks yang terdapat slang words di dalamnya. required Returns: Type Description str Teks yang telah dimodifikasi (tanpa slang words di dalamnya). Examples: Mengganti setiap slang words yang ada di dalam teks menjadi bentuk yang lebih formal. >>> indoNLP . preprocessing . replace_slang ( \"emg siapa yg nanya?\" ) \"memang siapa yang bertanya?\"","title":"replace_slang()"},{"location":"api/preprocessing/#indoNLP.preprocessing.replace_word_elongation","text":"Mengganti word elongation yang terdapat pada sebuah teks. Definisi Word elongation adalah tindakan menambahkan huruf tambahan ke kata, biasanya terdapat di akhir kata, hal ini biasanya dilakukan agar terdengar lebih ceria, ramah, dan imut. Parameters: Name Type Description Default text str Teks yang terdapat word elongation di dalamnya. required Returns: Type Description str Teks yang telah ditransformasi (tanpa word elongation ). Examples: Mengganti setiap word elongation yang terdapat pada sebuah teks. >>> indoNLP . preprocessing . replace_word_elongation ( \"kenapaaa?\" ) \"kenapa?\"","title":"replace_word_elongation()"},{"location":"api/preprocessing/#indoNLP.preprocessing.pipeline","text":"Pipelining fungsi preprocessing. Parameters: Name Type Description Default pipe Sequence [ Callable [[ str ], str ]] Sequence dari fungsi - fungsi preprocessing indoNLP . required Returns: Type Description Callable [[ str ], str ] Callable pipeline. Examples: Pipelining beberapa fungsi preprocessing. >>> from indoNLP.preprocessing import pipeline , replace_word_elongation , replace_slang >>> pipe = pipeline ([ replace_word_elongation , replace_slang ]) >>> pipe ( \"Knp emg gk mw makan kenapaaa???\" ) \"kenapa memang enggak mau makan kenapa???\"","title":"pipeline()"},{"location":"api/preprocessing/#indoNLP.preprocessing.emoji_to_words","text":"Transformasi emoji yang ada di dalam teks menjadi kata - kata yang sesuai dengan emoji tersebut dalam Bahasa Indonesia. Parameters: Name Type Description Default text str Teks yang terdapat emoji di dalamnya. required lang str Kode bahasa, bahasa yang tersedia yaitu \"en\" (English) dan \"id\" (Bahasa Indonesia). 'id' use_alias bool Menggunakan alias translation, alias adalah terjemahan yang lebih spesifik terhadap emoji tersebut. Tidak setiap emoji memiliki alias dan use_alias hanya didukung untuk Bahasa Indonesia lang=\"id\" . False delimiter Tuple [ str , str ] Delimiter (pembatas) pada terjemahan emoji, berupa tupple dengan dua element string sebagai pembatas awal dan akhir. ('!', '!') Warning Jika use_alias == True and lang != \"id\" maka akan terjadi error. Returns: Type Description str Teks yang telah di transformasi atau tidak terdapat emoji di dalamnya dan telah digantikan str dengan kata - kata yang mengekspresikan emoji tersebut. Examples: Mentransformasi emoji kedalam Bahasa Indonesia. >>> indoNLP . preprocessing . emoji_to_words ( \"emoji \ud83d\ude00\ud83d\ude01\" ) \"emoji !wajah_gembira!!wajah_gembira_dengan_mata_bahagia!\" Mentransformasi emoji kebahasa Ingris. >>> indoNLP . preprocessing . emoji_to_words ( \"emoji \ud83d\ude00\ud83d\ude01\" , lang = \"en\" ) \"emoji !grinning_face!!beaming_face_with_smiling_eyes!\" Menggunakan alias. >>> indoNLP . preprocessing . emoji_to_words ( \"emoji \ud83d\ude00\" , use_alias = True ) \"emoji !wajah_gembira_bahagia_muka_senang!\" Menggunakan custom delimiter. >>> indoNLP . preprocessing . emoji_to_words ( \"emoji \ud83d\ude01\" , delimiter = ( \"^\" , \"$\" )) \"emoji ^wajah_gembira_dengan_mata_bahagia$\"","title":"emoji_to_words()"},{"location":"api/preprocessing/#indoNLP.preprocessing.words_to_emoji","text":"Transformasi kata - kata dengan kode emoji menjadi emoji. Parameters: Name Type Description Default text str Teks yang terdapat kata - kata dengan kode emoji di dalamnya. required lang str Kode bahasa, bahasa yang tersedia yaitu \"en\" (English) dan \"id\" (Bahasa Indonesia). 'id' use_alias bool Menggunakan alias translation, alias adalah terjemahan yang lebih spesifik terhadap emoji tersebut. Tidak setiap emoji memiliki alias dan use_alias hanya didukung untuk Bahasa Indonesia lang=\"id\" . False delimiter Tuple [ str , str ] Delimiter (pembatas) pada kata - kata kode emoji, berupa tupple dengan dua element string sebagai pembatas awal dan akhir. ('!', '!') Returns: Type Description str Teks yang telah di transformasi atau kata - kata yang mengandung kode emoji di dalam teks str telah diubah menjadi emooji. Examples: Transformasi kata - kata kode emoji di dalam teks menjadi emoji. >>> indoNLP . preprocessing . emoji_to_words ( \"emoji !wajah_gembira!!wajah_gembira_dengan_mata_bahagia!\" ) \"emoji \ud83d\ude00\ud83d\ude01\" Transform english words to emoji >>> indoNLP . preprocessing . emoji_to_words ( \"emoji !beaming_face_with_smiling_eyes!\" , lang = \"en\" ) \"emoji \ud83d\ude01\" Using alias. Only works on lang == \"id\" >>> indoNLP . preprocessing . emoji_to_words ( \"emoji !wajah_gembira_bahagia_muka_senang!\" , use_alias = True ) \"emoji \ud83d\ude00\" Using custom delimiter >>> indoNLP . preprocessing . emoji_to_words ( \"emoji ^wajah_gembira_dengan_mata_bahagia$\" , delimiter = ( \"^\" , \"$\" )) \"emoji \ud83d\ude01\"","title":"words_to_emoji()"},{"location":"development/","text":"Development Untuk development indoNLP menggunakan python-poetry untuk packaging dan management dependencies. Install python-poetry Silahkan kunjungi installation untuk melihat cara installasi dari python-poetry berdasarkan OS yang digunakan. Setup Setup development environment, dengan menggunakan command $ make setup-dev Command tersebut akan menginstall semua dependencies yang digunakan oleh indoNLP dalam tahap development. Makefile Commands Commands yang terdapat pada Makefile digunakan untuk memudahkan proses development yaitu: setup-dev digunakan untuk setup development environment. format digunakan untuk mengformat menggunakan black dan isort . format-check digunakan untuk melihat apakah project telah mengikuti ketentuan black dan isort . typecheck digunakan untuk type checking menggunakan mypy test digunakan untuk melakukan testing menggunakan pytest Coverage Target Code coverage yang ditargetkan pada indoNLP adalah lebih dari 95%. pre-commit Sebelum melakukan commit pastikan kode lolos format-check dan typecheck karena akan diujikan oleh pre-commit , jika tidak lolos maka commit akan ditolak. Perhatian Pastikan anda berada dalam environment poetry saat melakukan commit , cara mengaktifkannya adalah dengan menggunakan command poetry shell","title":"Development"},{"location":"development/#development","text":"Untuk development indoNLP menggunakan python-poetry untuk packaging dan management dependencies. Install python-poetry Silahkan kunjungi installation untuk melihat cara installasi dari python-poetry berdasarkan OS yang digunakan.","title":"Development"},{"location":"development/#setup","text":"Setup development environment, dengan menggunakan command $ make setup-dev Command tersebut akan menginstall semua dependencies yang digunakan oleh indoNLP dalam tahap development.","title":"Setup"},{"location":"development/#makefile-commands","text":"Commands yang terdapat pada Makefile digunakan untuk memudahkan proses development yaitu: setup-dev digunakan untuk setup development environment. format digunakan untuk mengformat menggunakan black dan isort . format-check digunakan untuk melihat apakah project telah mengikuti ketentuan black dan isort . typecheck digunakan untuk type checking menggunakan mypy test digunakan untuk melakukan testing menggunakan pytest","title":"Makefile Commands"},{"location":"development/#coverage-target","text":"Code coverage yang ditargetkan pada indoNLP adalah lebih dari 95%.","title":"Coverage Target"},{"location":"development/#pre-commit","text":"Sebelum melakukan commit pastikan kode lolos format-check dan typecheck karena akan diujikan oleh pre-commit , jika tidak lolos maka commit akan ditolak. Perhatian Pastikan anda berada dalam environment poetry saat melakukan commit , cara mengaktifkannya adalah dengan menggunakan command poetry shell","title":"pre-commit"},{"location":"development/new-sup-dataset/","text":"Request Penambahan Supported Dataset Menambahkan Dataset Tambahkan informasi terkait dataset pada file indoNLP/dataset/list.py dengan ketentuan sebagai berikut: ... , \"{{ ID-DATASET-BARU }}\" : { \"info\" : { \"description\" : str , # Deskripsi singkat tentang dataset \"author\" : str , # Orang - orang yang memiliki hak cipta terhadap dataset \"year\" : int , # Tahun dataset dipublish \"citation\" : str , # Cara mengutip dataset \"homepage\" : str , # Website atau halaman utama dataset \"tags\" : List [ str ], # Tag - tag yang berhubungan dengan dataset }, \"files\" : [ # Berisi file - file yang terdapat dalam dataset { \"filename\" : str , # Nama file \"url\" : str , # URL atau endpoint tempat file dapat didownload \"is_large\" : bool , # Apakah ukuran file besar? \"extract\" : bool , # Apakah file perlu dilakukan ekstraksi? }, ... ], \"reader\" : { # Berisi keterangan tentang semua file yang terdapat di dataset \"{{ ID-FILE }}\" : { # id file dalam dataset agar dapat dikenali oleh method .read \"path\" : str , # path ke file yang akan dibaca relative terhadap `downloader.dataset_dir` \"is_table\" : bool , # Apakah data dalam file bersifat simetrik? \"reader\" : Callable , # Fungsi yang digunakan untuk membaca data pada file terdapat # pada indoNLP/dataset/reader.py jika tidak terdapat fungsi yang # tersedia maka buat fungsi baru dengan format yang sama terhadap # fungsi reader yang lain [TANPA TAMBAHAN DEPENDENCIES]. \"args\" : Dict , # kwargs yang perlu dipass kefungsi reader. }, ... }, }, } Ketentuan Jika diperlukan untuk menambah fungsi reader baru pastikan untuk menambakan juga test case pada file tests/dataset/test_reader.py untuk unit testing dan juga memperhatikan ketentuan code coverage . Warning Dalam pembuatan fungsi reader baru utamakan tidak menggunakan dependensi tambahan selain python standard library . Membuat Pull Request Setelah semua ketentuan tercapai buat Pull Request di repository indoNLP , akan dilakukan review apakah dataset dapat ditambahkan atau tidak.","title":"Penambahan Supported Dataset"},{"location":"development/new-sup-dataset/#request-penambahan-supported-dataset","text":"","title":"Request Penambahan Supported Dataset"},{"location":"development/new-sup-dataset/#menambahkan-dataset","text":"Tambahkan informasi terkait dataset pada file indoNLP/dataset/list.py dengan ketentuan sebagai berikut: ... , \"{{ ID-DATASET-BARU }}\" : { \"info\" : { \"description\" : str , # Deskripsi singkat tentang dataset \"author\" : str , # Orang - orang yang memiliki hak cipta terhadap dataset \"year\" : int , # Tahun dataset dipublish \"citation\" : str , # Cara mengutip dataset \"homepage\" : str , # Website atau halaman utama dataset \"tags\" : List [ str ], # Tag - tag yang berhubungan dengan dataset }, \"files\" : [ # Berisi file - file yang terdapat dalam dataset { \"filename\" : str , # Nama file \"url\" : str , # URL atau endpoint tempat file dapat didownload \"is_large\" : bool , # Apakah ukuran file besar? \"extract\" : bool , # Apakah file perlu dilakukan ekstraksi? }, ... ], \"reader\" : { # Berisi keterangan tentang semua file yang terdapat di dataset \"{{ ID-FILE }}\" : { # id file dalam dataset agar dapat dikenali oleh method .read \"path\" : str , # path ke file yang akan dibaca relative terhadap `downloader.dataset_dir` \"is_table\" : bool , # Apakah data dalam file bersifat simetrik? \"reader\" : Callable , # Fungsi yang digunakan untuk membaca data pada file terdapat # pada indoNLP/dataset/reader.py jika tidak terdapat fungsi yang # tersedia maka buat fungsi baru dengan format yang sama terhadap # fungsi reader yang lain [TANPA TAMBAHAN DEPENDENCIES]. \"args\" : Dict , # kwargs yang perlu dipass kefungsi reader. }, ... }, }, }","title":"Menambahkan Dataset"},{"location":"development/new-sup-dataset/#ketentuan","text":"Jika diperlukan untuk menambah fungsi reader baru pastikan untuk menambakan juga test case pada file tests/dataset/test_reader.py untuk unit testing dan juga memperhatikan ketentuan code coverage . Warning Dalam pembuatan fungsi reader baru utamakan tidak menggunakan dependensi tambahan selain python standard library .","title":"Ketentuan"},{"location":"development/new-sup-dataset/#membuat-pull-request","text":"Setelah semua ketentuan tercapai buat Pull Request di repository indoNLP , akan dilakukan review apakah dataset dapat ditambahkan atau tidak.","title":"Membuat Pull Request"}]}